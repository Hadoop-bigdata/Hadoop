Apache Hadoop is an open source software framework. It provides a distributed environment for storing and processing the data. It provides the ability to scale from one single machine to clusters of computers. The two modules leveraged in this tutorial are Hadoop Distributed File System(HDFS) and Hadoop MapReduce. HDFS gives the fast speed for data access. And MapReduce is a YARN based system for parallel processing. Therefore it meets up with the need for the big data processing. 

![alt text](https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=0ahUKEwiJ7_uvpo_YAhUDbRQKHWLWB8EQjRwIBw&url=https%3A%2F%2Fwww.stackchief.com%2Fblog%2FAn%2520Introduction%2520to%2520Apache%2520Hadoop&psig=AOvVaw14KfbcW49W9ZYbhQXgv9mU&ust=1513540081308524)

![hadoop-elephant_logo](https://user-images.githubusercontent.com/26347639/34073834-0167b5fe-e271-11e7-8974-0f4850969a7b.png)

MapReduce is a technique for distributed computing. Two main tasks are contained here: Map and Reduce. The Map comes before Reduce. Map basically converts a dataset to a set of data containing tuples(key/value) of single element. After Map finishes its job. Reduce takes the output to combine them into smaller number of tuples.








